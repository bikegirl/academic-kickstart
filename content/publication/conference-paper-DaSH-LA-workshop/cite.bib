@inproceedings{iglesias-flores-etal-2021-topgunn,
    title = "{T}op{G}u{NN}: Fast {NLP} Training Data Augmentation using Large Corpora",
    author = "Iglesias-Flores, Rebecca  and
      Mishra, Megha  and
      Patel, Ajay  and
      Malhotra, Akanksha  and
      Kriz, Reno  and
      Palmer, Martha  and
      Callison-Burch, Chris",
    booktitle = "Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.dash-1.14",
    doi = "10.18653/v1/2021.dash-1.14",
    pages = "86--101",
    abstract = "Acquiring training data for natural language processing systems can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present TopGuNN, a fast contextualized k-NN retrieval system that can efficiently index and search over contextual embeddings generated from large corpora. TopGuNN is demonstrated for a training data augmentation use case over the Gigaword corpus. Using approximate k-NN and an efficient architecture, TopGuNN performs queries over an embedding space of 4.63TB (approximately 1.5B embeddings) in less than a day.",
}
