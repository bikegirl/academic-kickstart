---
title: TopGuNN
summary: Fast NLP Training Data Augmentation using Large Corpora!
tags:
- Data Augmentation
date: "2021-06-11T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: "https://youtu.be/mPIE2mwMCVU"

image:
  caption: Photo by Mens Journal
  focal_point: Smart

links:
- icon: twitter
  icon_pack: fab
  name: Follow
  url: https://twitter.com/tensorglow
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: example
---

&emsp;&emsp;&emsp;Acquiring training data for natural language processing systems can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present TopGuNN, a fast contextualized k-NN retrieval system that can efficiently index and search over contextual embeddings generated from large corpora. TopGuNN is demonstrated for a training data augmentation use case over the Gigaword corpus. Using approximate k-NN and an efficient architecture, TopGuNN performs queries over an embedding space of 4.63TB (approximately 1.5B embeddings) in less than a day.
<br>
<br>
<br>

{{< youtube mPIE2mwMCVU >}}

<br>
<br>
<br>
